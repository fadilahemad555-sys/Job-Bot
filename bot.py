import os
import requests
import json
import time
import re
from bs4 import BeautifulSoup
from urllib.parse import quote

# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==========
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
SENT_FILE = "sent_links.json"

# ========== ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© (ÙƒÙ…Ø§ ÙÙŠ Google Alerts) ==========
KEYWORDS_QUERY = '''
("looking for" OR "need" OR "wanted" OR "seeking" OR "searching for" OR 
"Ù…Ø±Ø­Ø¨Ø§ Ø£Ø­ØªØ§Ø¬ Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ" OR "Ø£Ø­ØªØ§Ø¬ Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ" OR "Ù„Ø¯ÙŠ Ø¨Ø¹Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø£Ø±ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§") 
("video editor" OR "reels editor" OR "short form editor" OR "create video from text" OR 
"Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ" OR "Ù…Ø­Ø±Ø± Ø±ÙŠÙ„Ø²") 
("DM me" OR "contact me" OR "send DM" OR "shoot me a DM" OR "Ø±Ø§Ø³Ù„Ù†ÙŠ" OR "Ø§Ù„Ø®Ø§Øµ" OR "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ") 
(site:twitter.com OR site:facebook.com OR site:instagram.com OR site:youtube.com OR site:tiktok.com) 
-"job" -"hiring" -"career" -"vacancy" -"apply" -"recruitment" -"linkedin" -"indeed" -"fiverr" -"upwork" -"freelancer" -"guru" -"glassdoor" -"bayt"
'''

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹)
KEYWORDS = [
    "looking for video editor",
    "need video editor",
    "wanted video editor",
    "seeking video editor",
    "searching for video editor",
    "Ù…Ø±Ø­Ø¨Ø§ Ø£Ø­ØªØ§Ø¬ Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ",
    "Ø£Ø­ØªØ§Ø¬ Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ",
    "Ù„Ø¯ÙŠ Ø¨Ø¹Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø£Ø±ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§",
    "video editor",
    "reels editor",
    "short form editor",
    "create video from text",
    "Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ",
    "Ù…Ø­Ø±Ø± Ø±ÙŠÙ„Ø²"
]

# ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯
EXCLUDE_WORDS = ["job", "hiring", "career", "vacancy", "apply", "recruitment", 
                 "linkedin", "indeed", "fiverr", "upwork", "freelancer", "guru", 
                 "glassdoor", "bayt"]

# Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
SITES = ["twitter.com", "facebook.com", "instagram.com", "youtube.com", "tiktok.com"]

# ========== Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ==========
def load_sent():
    if os.path.exists(SENT_FILE):
        with open(SENT_FILE, 'r') as f:
            return set(json.load(f))
    return set()

def save_sent(sent_set):
    with open(SENT_FILE, 'w') as f:
        json.dump(list(sent_set), f)

def send_telegram(message):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {'chat_id': CHAT_ID, 'text': message, 'parse_mode': 'HTML', 'disable_web_page_preview': True}
    try:
        requests.post(url, json=payload, timeout=10)
    except Exception as e:
        print("ÙØ´Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„:", e)

def is_excluded(text):
    text_lower = text.lower()
    for word in EXCLUDE_WORDS:
        if word in text_lower:
            return True
    return False

# ========== Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø±ÙŠØ¯ÙŠØª (Ù…ØµØ¯Ø± Ø¥Ø¶Ø§ÙÙŠ) ==========
def search_reddit(sent_links):
    headers = {'User-Agent': 'Mozilla/5.0'}
    subreddits = ["forhire", "jobs", "freelance", "videoediting", "videography"]
    found = False
    for sub in subreddits:
        for kw in KEYWORDS:
            try:
                url = f"https://www.reddit.com/r/{sub}/search.json?q={kw}&restrict_sr=on&sort=new&limit=5"
                res = requests.get(url, headers=headers, timeout=10)
                if res.status_code == 200:
                    data = res.json()
                    posts = data.get('data', {}).get('children', [])
                    for post in posts:
                        p = post['data']
                        title = p['title']
                        permalink = p['permalink']
                        full_url = f"https://reddit.com{permalink}"
                        if full_url not in sent_links and not is_excluded(title):
                            sent_links.add(full_url)
                            msg = f"ğŸ”´ <b>Reddit r/{sub}</b>\n\n{title}\n\nğŸ”— {full_url}"
                            send_telegram(msg)
                            found = True
                            time.sleep(1)
            except:
                continue
    return found

# ========== Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ØªÙˆÙŠØªØ± Ø¹Ø¨Ø± Nitter ==========
def search_twitter(sent_links):
    headers = {'User-Agent': 'Mozilla/5.0'}
    found = False
    for kw in KEYWORDS:
        try:
            url = f"https://nitter.net/search?q={kw}&f=tweets"
            res = requests.get(url, headers=headers, timeout=10)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                tweets = soup.find_all('div', class_='tweet-content')[:10]
                for tweet in tweets:
                    text = tweet.get_text(strip=True)
                    link_tag = tweet.find_parent('a', href=True)
                    if link_tag:
                        link = "https://nitter.net" + link_tag['href']
                        if link not in sent_links and not is_excluded(text):
                            sent_links.add(link)
                            msg = f"ğŸ¦ <b>ØªÙˆÙŠØªØ±</b>\n\n{text[:200]}...\n\nğŸ”— {link}"
                            send_telegram(msg)
                            found = True
                            time.sleep(1)
        except:
            continue
    return found

# ========== Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…ÙˆØ§Ù‚Ø¹ Ø£Ø®Ø±Ù‰ Ø¹Ø¨Ø± Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„ (Ù…Ø­Ø§ÙƒØ§Ø© Ø¨Ø³ÙŠØ·Ø©) ==========
def search_google(sent_links):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    found = False
    for site in SITES:
        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø«: ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© + site: + Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ ÙƒÙ„Ù…Ø§Øª
        query = f'({" OR ".join(KEYWORDS[:5])}) site:{site} -{" -".join(EXCLUDE_WORDS)}'
        url = f"https://www.google.com/search?q={quote(query)}&tbs=qdr:d"  # Ø¢Ø®Ø± ÙŠÙˆÙ…
        try:
            res = requests.get(url, headers=headers, timeout=10)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù‚Ø¯ ÙŠØªØºÙŠØ± Ø¨Ù†Ø§Ø¡ Ø¬ÙˆØ¬Ù„)
                results = soup.find_all('div', class_='g')[:5]
                for r in results:
                    link_tag = r.find('a', href=True)
                    if link_tag:
                        link = link_tag['href']
                        if link.startswith('/url?q='):
                            link = link.split('/url?q=')[1].split('&')[0]
                        if link not in sent_links and not is_excluded(r.get_text()):
                            sent_links.add(link)
                            title = r.find('h3')
                            title_text = title.get_text() if title else "Ù†ØªÙŠØ¬Ø© Ø¨Ø­Ø«"
                            msg = f"ğŸŒ <b>{site}</b>\n\n{title_text}\n\nğŸ”— {link}"
                            send_telegram(msg)
                            found = True
                            time.sleep(2)
        except:
            continue
    return found

# ========== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==========
if __name__ == "__main__":
    if not BOT_TOKEN or not CHAT_ID:
        print("âŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ¹ÙŠÙŠÙ† BOT_TOKEN Ùˆ CHAT_ID ÙÙŠ Ø§Ù„Ø£Ø³Ø±Ø§Ø±")
        exit()

    sent_links = load_sent()
    send_telegram("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ø±ÙŠØ¯ÙŠØª + ØªÙˆÙŠØªØ± + Ø¬ÙˆØ¬Ù„)...")

    found_reddit = search_reddit(sent_links)
    found_twitter = search_twitter(sent_links)
    found_google = search_google(sent_links)

    if found_reddit or found_twitter or found_google:
        save_sent(sent_links)
        send_telegram("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø¬Ø¯ÙŠØ¯Ø©!")
    else:
        send_telegram("â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø¬Ø¯ÙŠØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")